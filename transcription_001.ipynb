{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"./241118_1543.mp3\"\n",
    "output_file = input_file.replace(\".mp3\", \".wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport ffmpeg\\n\\ntry:\\n    stream = ffmpeg.input(input_file)\\n    stream = ffmpeg.output(stream, output_file)\\n    ffmpeg.run(stream)\\n    print(\"Conversion successful\")\\nexcept ffmpeg.Error as e:\\n    print(f\"Error: {e.stderr.decode()}\")\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import ffmpeg\n",
    "\n",
    "try:\n",
    "    stream = ffmpeg.input(input_file)\n",
    "    stream = ffmpeg.output(stream, output_file)\n",
    "    ffmpeg.run(stream)\n",
    "    print(\"Conversion successful\")\n",
    "except ffmpeg.Error as e:\n",
    "    print(f\"Error: {e.stderr.decode()}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# To save your Huggingface token, run your terminal:\n",
    "# echo 'export HF_TOKEN=\"hf_*******************************\"' >> $HOME/.bashrc\n",
    "\n",
    "# Otherwise, the login function will prompt a login interface\n",
    "login(token=os.environ.get(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.6.0. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nmassari/ghe_local/venv_transcribe/lib/python3.13/site-packages/speechbrain/utils/autocast.py:68: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n",
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
      "/Users/nmassari/ghe_local/venv_transcribe/lib/python3.13/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription results saved to 'transcription_results.csv'\n"
     ]
    }
   ],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "import torch\n",
    "import whisper\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Initialize Pyannote pipeline\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\")\n",
    "\n",
    "# Load audio file\n",
    "audio_file = output_file\n",
    "\n",
    "# Check if MPS is available and set device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "# Send pipeline to the appropriate device\n",
    "pipeline.to(device)\n",
    "\n",
    "# Perform diarization\n",
    "diarization = pipeline(audio_file)\n",
    "\n",
    "# Load Whisper model and transcribe audio\n",
    "model = whisper.load_model(\"base.en\")\n",
    "result = model.transcribe(audio_file)\n",
    "\n",
    "# Process diarization and transcription results\n",
    "transcript = {}\n",
    "assigned_sections = set()  # To track assigned sections\n",
    "\n",
    "def calculate_overlap(start1, end1, start2, end2):\n",
    "    \"\"\"Calculate the overlap between two time intervals.\"\"\"\n",
    "    overlap_start = max(start1, start2)\n",
    "    overlap_end = min(end1, end2)\n",
    "    return max(0, overlap_end - overlap_start)\n",
    "\n",
    "# Store overlaps for each section\n",
    "for segment, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    segment_duration = segment.end - segment.start\n",
    "    if speaker not in transcript:\n",
    "        transcript[speaker] = []\n",
    "\n",
    "    for section in result[\"segments\"]:\n",
    "        overlap = calculate_overlap(segment.start, segment.end, section[\"start\"], section[\"end\"])\n",
    "        overlap_fraction = overlap / (section[\"end\"] - section[\"start\"])\n",
    "\n",
    "        # Store overlap details\n",
    "        section.setdefault(\"overlaps\", [])\n",
    "        section[\"overlaps\"].append((speaker, overlap, overlap_fraction))\n",
    "\n",
    "# Assign text based on overlap conditions\n",
    "for section in result[\"segments\"]:\n",
    "    if \"overlaps\" in section and section[\"id\"] not in assigned_sections:\n",
    "        overlaps = sorted(section[\"overlaps\"], key=lambda x: x[1], reverse=True)  # Sort by overlap amount\n",
    "        max_overlap = overlaps[0]\n",
    "\n",
    "        if max_overlap[2] > 0.5:\n",
    "            # Assign to the segment with >50% overlap\n",
    "            speaker = max_overlap[0]\n",
    "            transcript[speaker].append((section[\"start\"], section[\"end\"], section[\"text\"]))\n",
    "            assigned_sections.add(section[\"id\"])\n",
    "        elif len(overlaps) > 1:\n",
    "            # Assign to the segment with the most overlap if all are <50%\n",
    "            speaker = max_overlap[0]\n",
    "            transcript[speaker].append((section[\"start\"], section[\"end\"], section[\"text\"]))\n",
    "            assigned_sections.add(section[\"id\"])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame([(speaker, start, end, text) \n",
    "                   for speaker, segments in transcript.items() \n",
    "                   for start, end, text in segments],\n",
    "                  columns=[\"Speaker\", \"Start\", \"End\", \"Text\"])\n",
    "\n",
    "# Sort by start time and save to CSV\n",
    "df.sort_values(by=\"Start\").reset_index(drop=True).to_csv(\"transcription_results.csv\", index=False)\n",
    "\n",
    "print(\"Transcription results saved to 'transcription_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLOWER () pipeline using https://github.com/yinruiqing/pyannote-whisper\n",
    "\"\"\"\n",
    "from pyannote.audio import Pipeline\n",
    "import torch\n",
    "import utils # CREDIT: https://github.com/yinruiqing/pyannote-whisper\n",
    "import whisper\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Pyannote pipeline\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\")\n",
    "\n",
    "# Load audio file\n",
    "audio_file = output_file\n",
    "\n",
    "model = whisper.load_model(\"base.en\")\n",
    "asr_result = model.transcribe(audio_file)\n",
    "diarization_result = pipeline(audio_file)\n",
    "final_result = utils.diarize_text(asr_result, diarization_result)\n",
    "\n",
    "for seg, spk, sent in final_result:\n",
    "    line = f'{seg.start:.2f} {seg.end:.2f} {spk} {sent}'\n",
    "    print(line)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_transcribe",
   "language": "python",
   "name": "venv_transcribe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
