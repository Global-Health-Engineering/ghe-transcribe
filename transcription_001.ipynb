{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"./241118_1543.mp3\"\n",
    "output_file = input_file.replace(\".mp3\", \".wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport ffmpeg\\n\\ntry:\\n    stream = ffmpeg.input(input_file)\\n    stream = ffmpeg.output(stream, output_file)\\n    ffmpeg.run(stream)\\n    print(\"Conversion successful\")\\nexcept ffmpeg.Error as e:\\n    print(f\"Error: {e.stderr.decode()}\")\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import ffmpeg\n",
    "\n",
    "try:\n",
    "    stream = ffmpeg.input(input_file)\n",
    "    stream = ffmpeg.output(stream, output_file)\n",
    "    ffmpeg.run(stream)\n",
    "    print(\"Conversion successful\")\n",
    "except ffmpeg.Error as e:\n",
    "    print(f\"Error: {e.stderr.decode()}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# To save your Huggingface token, run your terminal:\n",
    "# echo 'export HF_TOKEN=\"hf_*******************************\"' >> $HOME/.bashrc\n",
    "\n",
    "# Otherwise, the login function will prompt a login interface\n",
    "login(token=os.environ.get(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.6.0. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<pyannote.core.annotation.Annotation at 0x153536c10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "import torch\n",
    "import whisper\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Pyannote pipeline\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\")\n",
    "\n",
    "# Load audio file\n",
    "audio_file = output_file\n",
    "\n",
    "# Check if MPS is available and set device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "# Send pipeline to the appropriate device\n",
    "pipeline.to(device)\n",
    "\n",
    "# Perform diarization\n",
    "who_speaks_when = pipeline(audio_file)\n",
    "\n",
    "\"\"\"\n",
    "# Load Whisper model and transcribe audio\n",
    "model = whisper.load_model(\"base.en\")\n",
    "result = model.transcribe(audio_file)\n",
    "\n",
    "# Process who_speaks_when and transcription results\n",
    "transcript = {}\n",
    "assigned_sections = set()  # To track assigned sections\n",
    "\n",
    "def calculate_overlap(start1, end1, start2, end2):\n",
    "    #Calculate the overlap between two time intervals.\n",
    "    overlap_start = max(start1, start2)\n",
    "    overlap_end = min(end1, end2)\n",
    "    return max(0, overlap_end - overlap_start)\n",
    "\n",
    "# Store overlaps for each section\n",
    "for segment, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    segment_duration = segment.end - segment.start\n",
    "    if speaker not in transcript:\n",
    "        transcript[speaker] = []\n",
    "\n",
    "    for section in result[\"segments\"]:\n",
    "        overlap = calculate_overlap(segment.start, segment.end, section[\"start\"], section[\"end\"])\n",
    "        overlap_fraction = overlap / (section[\"end\"] - section[\"start\"])\n",
    "\n",
    "        # Store overlap details\n",
    "        section.setdefault(\"overlaps\", [])\n",
    "        section[\"overlaps\"].append((speaker, overlap, overlap_fraction))\n",
    "\n",
    "# Assign text based on overlap conditions\n",
    "for section in result[\"segments\"]:\n",
    "    if \"overlaps\" in section and section[\"id\"] not in assigned_sections:\n",
    "        overlaps = sorted(section[\"overlaps\"], key=lambda x: x[1], reverse=True)  # Sort by overlap amount\n",
    "        max_overlap = overlaps[0]\n",
    "\n",
    "        if max_overlap[2] > 0.5:\n",
    "            # Assign to the segment with >50% overlap\n",
    "            speaker = max_overlap[0]\n",
    "            transcript[speaker].append((section[\"start\"], section[\"end\"], section[\"text\"]))\n",
    "            assigned_sections.add(section[\"id\"])\n",
    "        elif len(overlaps) > 1:\n",
    "            # Assign to the segment with the most overlap if all are <50%\n",
    "            speaker = max_overlap[0]\n",
    "            transcript[speaker].append((section[\"start\"], section[\"end\"], section[\"text\"]))\n",
    "            assigned_sections.add(section[\"id\"])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame([(speaker, start, end, text) \n",
    "                   for speaker, segments in transcript.items() \n",
    "                   for start, end, text in segments],\n",
    "                  columns=[\"Speaker\", \"Start\", \"End\", \"Text\"])\n",
    "\n",
    "# Sort by start time and save to CSV\n",
    "df.sort_values(by=\"Start\").reset_index(drop=True).to_csv(\"transcription_results.csv\", index=False)\n",
    "\n",
    "print(\"Transcription results saved to 'transcription_results.csv'\")\n",
    "\"\"\"\n",
    "\n",
    "diarization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLOWER () pipeline using https://github.com/yinruiqing/pyannote-whisper\n",
    "\"\"\"\n",
    "from pyannote.audio import Pipeline\n",
    "import torch\n",
    "import utils # CREDIT: https://github.com/yinruiqing/pyannote-whisper\n",
    "import whisper\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Pyannote pipeline\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\")\n",
    "\n",
    "# Load audio file\n",
    "audio_file = output_file\n",
    "\n",
    "model = whisper.load_model(\"base.en\")\n",
    "asr_result = model.transcribe(audio_file)\n",
    "diarization_result = pipeline(audio_file)\n",
    "final_result = utils.diarize_text(asr_result, diarization_result)\n",
    "\n",
    "for seg, spk, sent in final_result:\n",
    "    line = f'{seg.start:.2f} {seg.end:.2f} {spk} {sent}'\n",
    "    print(line)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.6.0. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nmassari/ghe_local/venv_transcribe/lib/python3.13/site-packages/speechbrain/utils/autocast.py:68: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n",
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<pyannote.core.annotation.Annotation at 0x143979940>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From https://gist.github.com/hbredin/049f2b629700bcea71324d2c1e7f8337\n",
    "from pyannote.audio import Pipeline\n",
    "import torch\n",
    "\n",
    "# Check if MPS is available and set device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load pyannote.audio speaker diarization \n",
    "from pyannote.audio import Pipeline\n",
    "speaker_diarization = Pipeline.from_pretrained(\"pyannote/speaker-diarization@2.1\", \n",
    "                                               use_auth_token=True).to(device)\n",
    "     \n",
    "# Load audio file\n",
    "audio_file = output_file\n",
    "\n",
    "# apply speaker diarization\n",
    "who_speaks_when = speaker_diarization(audio_file, \n",
    "                                      num_speakers=None,  # these values can be\n",
    "                                      min_speakers=None,  # provided by the user\n",
    "                                      max_speakers=None)  # when they are known\n",
    "     \n",
    "\n",
    "# reset notebook visualization (including start time, end time and speaker colors)\n",
    "from pyannote.core import notebook\n",
    "notebook.reset()\n",
    "\n",
    "who_speaks_when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nmassari/ghe_local/venv_transcribe/lib/python3.13/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# load OpenAI Whisper automatic speech transcription \n",
    "import whisper\n",
    "# choose among \"tiny\", \"base\", \"small\", \"medium\", \"large\"\n",
    "# see https://github.com/openai/whisper/\n",
    "model = whisper.load_model(\"small\") \n",
    "text = model.transcribe(audio_file)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 48870])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nmassari/ghe_local/venv_transcribe/lib/python3.13/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(waveform.shape)\n\u001b[32m     21\u001b[39m waveform_tensor = torch.from_numpy(waveform.squeeze().numpy())\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m text = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform_tensor\u001b[49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msegment.start\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m06.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msegment.end\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m06.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspeaker\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ghe_local/venv_transcribe/lib/python3.13/site-packages/whisper/transcribe.py:153\u001b[39m, in \u001b[36mtranscribe\u001b[39m\u001b[34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, carry_initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, clip_timestamps, hallucination_silence_threshold, **decode_options)\u001b[39m\n\u001b[32m    151\u001b[39m mel_segment = pad_or_trim(mel, N_FRAMES).to(model.device).to(dtype)\n\u001b[32m    152\u001b[39m _, probs = model.detect_language(mel_segment)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m decode_options[\u001b[33m\"\u001b[39m\u001b[33mlanguage\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mmax\u001b[39m(probs, key=\u001b[43mprobs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m)\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    155\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDetected language: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLANGUAGES[decode_options[\u001b[33m'\u001b[39m\u001b[33mlanguage\u001b[39m\u001b[33m'\u001b[39m]].title()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m     )\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# load OpenAI Whisper automatic speech transcription \n",
    "import whisper\n",
    "\n",
    "# choose among \"tiny\", \"base\", \"small\", \"medium\", \"large\"\n",
    "# see https://github.com/openai/whisper/\n",
    "model = whisper.load_model(\"small\") \n",
    "     \n",
    "\n",
    "# transcribing first minute\n",
    "from pyannote.core import Segment\n",
    "first_minute = Segment(0, 60)\n",
    "\n",
    "from pyannote.audio import Audio\n",
    "audio = Audio(sample_rate=16000, mono=True)\n",
    "\n",
    "import torch\n",
    "\n",
    "for segment, _, speaker in who_speaks_when.crop(first_minute).itertracks(yield_label=True):\n",
    "    waveform, sample_rate = audio.crop(audio_file, segment)\n",
    "    print(waveform.shape)\n",
    "    waveform_tensor = torch.from_numpy(waveform.squeeze().numpy())\n",
    "    text = model.transcribe(waveform_tensor)[\"text\"]\n",
    "    print(f\"{segment.start:06.1f}s {segment.end:06.1f}s {speaker}: {text}\")\"\n",
    "    \"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nmassari/ghe_local/venv_transcribe/lib/python3.13/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.0s 0004.0s SPEAKER_01:  So welcome to this interview today. I'm sitting here with Natalie.\n",
      "0005.0s 0014.5s SPEAKER_01:  And we're going to have just a little interview with two questions. My name is Lars and I'm handing over to my interview to introduce yourself.\n",
      "0015.2s 0020.9s SPEAKER_00:  Thank you. My name is Natalie. I'm an employee at GIG and we are testing this new device.\n",
      "0022.4s 0027.0s SPEAKER_01:  Okay, thank you. My first question to you is...\n",
      "0027.7s 0029.3s SPEAKER_01:  When is your next field trip?\n",
      "0031.0s 0034.9s SPEAKER_00:  I am not sure. We are hoping for me.\n",
      "0034.7s 0038.9s SPEAKER_01:  Okay, you're hoping for me. I think I'm bad. When was your last field trip?\n",
      "0035.6s 0036.0s SPEAKER_00:  Bye.\n",
      "0039.0s 0040.5s SPEAKER_00:  I lost my angel lot.\n",
      "0041.9s 0043.3s SPEAKER_01:  Okay, thank you very much for the interview.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "from pyannote.core import Segment\n",
    "from pyannote.audio import Audio\n",
    "\n",
    "# Load the Whisper model\n",
    "model = whisper.load_model(\"small\")\n",
    "\n",
    "# Transcribing first minute\n",
    "first_minute = Segment(0, 60)\n",
    "audio = Audio(sample_rate=16000, mono=True)\n",
    "\n",
    "for segment, _, speaker in who_speaks_when.crop(first_minute).itertracks(yield_label=True):\n",
    "    waveform, sample_rate = audio.crop(audio_file, segment)\n",
    "    \n",
    "    # Convert to mono by averaging the two channels\n",
    "    if waveform.shape[0] == 2:  # Check if it's stereo\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)  # Average the two channels\n",
    "    \n",
    "    # Ensure waveform is a 1D tensor\n",
    "    waveform_tensor = waveform.squeeze()  # Remove any extra dimensions\n",
    "    \n",
    "    # Transcribe\n",
    "    text = model.transcribe(waveform_tensor.numpy())[\"text\"]  # Convert to numpy array\n",
    "    print(f\"{segment.start:06.1f}s {segment.end:06.1f}s {speaker}: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport utils \\n\\ntext = utils.diarize_text(transcribed, who_speaks_when)\\n\\nfor seg, spk, sent in text:\\n    line = f'{seg.start:.2f} {seg.end:.2f} {spk} {sent}'\\n    print(line)\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import utils \n",
    "\n",
    "text = utils.diarize_text(transcribed, who_speaks_when)\n",
    "\n",
    "for seg, spk, sent in text:\n",
    "    line = f'{seg.start:.2f} {seg.end:.2f} {spk} {sent}'\n",
    "    print(line)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process diarization and transcription results\n",
    "transcript = {}\n",
    "assigned_sections = set()  # To track assigned sections\n",
    "\n",
    "def calculate_overlap(start1, end1, start2, end2):\n",
    "    #Calculate the overlap between two time intervals.\n",
    "    overlap_start = max(start1, start2)\n",
    "    overlap_end = min(end1, end2)\n",
    "    return max(0, overlap_end - overlap_start)\n",
    "\n",
    "# Store overlaps for each section\n",
    "for segment, _, speaker in who_speaks_when.itertracks(yield_label=True):\n",
    "    segment_duration = segment.end - segment.start\n",
    "    if speaker not in transcript:\n",
    "        transcript[speaker] = []\n",
    "\n",
    "    for section in text[\"segments\"]:\n",
    "        overlap = calculate_overlap(segment.start, segment.end, section[\"start\"], section[\"end\"])\n",
    "        overlap_fraction = overlap / (section[\"end\"] - section[\"start\"])\n",
    "\n",
    "        # Store overlap details\n",
    "        section.setdefault(\"overlaps\", [])\n",
    "        section[\"overlaps\"].append((speaker, overlap, overlap_fraction))\n",
    "\n",
    "# Assign text based on overlap conditions\n",
    "for section in text[\"segments\"]:\n",
    "    if \"overlaps\" in section and section[\"id\"] not in assigned_sections:\n",
    "        overlaps = sorted(section[\"overlaps\"], key=lambda x: x[1], reverse=True)  # Sort by overlap amount\n",
    "        max_overlap = overlaps[0]\n",
    "\n",
    "        if max_overlap[2] > 0.5:\n",
    "            # Assign to the segment with >50% overlap\n",
    "            speaker = max_overlap[0]\n",
    "            transcript[speaker].append((section[\"start\"], section[\"end\"], section[\"text\"]))\n",
    "            assigned_sections.add(section[\"id\"])\n",
    "        elif len(overlaps) > 1:\n",
    "            # Assign to the segment with the most overlap if all are <50%\n",
    "            speaker = max_overlap[0]\n",
    "            transcript[speaker].append((section[\"start\"], section[\"end\"], section[\"text\"]))\n",
    "            assigned_sections.add(section[\"id\"])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame([(speaker, start, end, text) \n",
    "                   for speaker, segments in transcript.items() \n",
    "                   for start, end, text in segments],\n",
    "                  columns=[\"Speaker\", \"Start\", \"End\", \"Text\"])\n",
    "\n",
    "# Sort by start time and save to CSV\n",
    "df.sort_values(by=\"Start\").reset_index(drop=True).to_csv(\"transcription_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
