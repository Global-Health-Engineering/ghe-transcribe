{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# To save your Huggingface token, run your terminal:\n",
    "# echo 'export HF_TOKEN=\"hf_*******************************\"' >> $HOME/.bashrc\n",
    "\n",
    "# Otherwise, the login function will prompt a login interface\n",
    "login(token=os.environ.get(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.6.0. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nmassari/ghe_local/venv_transcribe/lib/python3.13/site-packages/speechbrain/utils/autocast.py:68: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n",
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
      "/Users/nmassari/ghe_local/venv_transcribe/lib/python3.13/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00 14.50 SPEAKER_01  So welcome to this interview today. I'm sitting here with Natalie and we're going to have just a little interview with two questions. My name is Lars and I'm handing over to my interviewee to introduce yourself.\n",
      "14.50 21.00 SPEAKER_00  Okay, thank you. My name is Natalie. I'm an employee at GAG and we are testing this new device.\n",
      "21.00 29.00 SPEAKER_01  Okay, thank you. My first question to you is when is your next field trip?\n",
      "29.00 34.00 SPEAKER_00  That is a great question Lars. I am not sure. We are hoping for me.\n",
      "34.00 39.00 SPEAKER_01  Can you open for me? When was your last field trip?\n",
      "39.00 41.00 SPEAKER_00  I last went in July.\n",
      "41.00 44.00 SPEAKER_01  Okay, thank you very much for the interview.\n"
     ]
    }
   ],
   "source": [
    "# FROM https://github.com/yinruiqing/pyannote-whisper\n",
    "\n",
    "from pyannote.audio import Pipeline\n",
    "import torch\n",
    "import utils # CREDIT: https://github.com/yinruiqing/pyannote-whisper\n",
    "import whisper\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize Pyannote pipeline\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization@2.1\").to(device)\n",
    "\n",
    "# Load audio file\n",
    "audio_file = \"241118_1543.wav\"\n",
    "\n",
    "model = whisper.load_model(\"base.en\")\n",
    "asr_result = model.transcribe(audio_file)\n",
    "diarization_result = pipeline(audio_file)\n",
    "final_result = utils.diarize_text(asr_result, diarization_result)\n",
    "\n",
    "for seg, spk, sent in final_result:\n",
    "    line = f'{seg.start:.2f} {seg.end:.2f} {spk} {sent}'\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.6.0. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n"
     ]
    }
   ],
   "source": [
    "# FROM https://gist.github.com/hbredin/049f2b629700bcea71324d2c1e7f8337\n",
    "\n",
    "from pyannote.audio import Pipeline\n",
    "import torch\n",
    "\n",
    "# Check if MPS is available and set device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load pyannote.audio speaker diarization \n",
    "from pyannote.audio import Pipeline\n",
    "speaker_diarization = Pipeline.from_pretrained(\"pyannote/speaker-diarization@2.1\", \n",
    "                                               use_auth_token=True).to(device)\n",
    "     \n",
    "# Load audio file\n",
    "audio_file = \"241118_1543.wav\"\n",
    "\n",
    "# apply speaker diarization\n",
    "who_speaks_when = speaker_diarization(audio_file, \n",
    "                                      num_speakers=None,  # these values can be\n",
    "                                      min_speakers=None,  # provided by the user\n",
    "                                      max_speakers=None)  # when they are known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nmassari/ghe_local/venv_transcribe/lib/python3.13/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.0s 0004.0s SPEAKER_01:  So welcome to this interview today. I'm sitting here with Natalie.\n",
      "0005.0s 0014.5s SPEAKER_01:  And we're going to have just a little interview with two questions. My name is Lars and I'm handing over to my interview to introduce yourself.\n",
      "0015.2s 0020.9s SPEAKER_00:  Thank you. My name is Natalie. I'm an employee at GIG and we are testing this new device.\n",
      "0022.4s 0027.0s SPEAKER_01:  Okay, thank you. My first question to you is...\n",
      "0027.7s 0029.3s SPEAKER_01:  When is your next field trip?\n",
      "0031.0s 0034.9s SPEAKER_00:  I am not sure. We are hoping for me.\n",
      "0034.7s 0038.9s SPEAKER_01:  Okay, you're hoping for me. I think I'm bad. When was your last field trip?\n",
      "0035.6s 0036.0s SPEAKER_00:  Bye.\n",
      "0039.0s 0040.5s SPEAKER_00:  I lost my angel lot.\n",
      "0041.9s 0043.3s SPEAKER_01:  Okay, thank you very much for the interview.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "from pyannote.core import Segment\n",
    "from pyannote.audio import Audio\n",
    "\n",
    "# Load the Whisper model\n",
    "model = whisper.load_model(\"small\")\n",
    "\n",
    "# Transcribing first minute\n",
    "first_minute = Segment(0, 60)\n",
    "audio = Audio(sample_rate=16000, mono=True)\n",
    "\n",
    "for segment, _, speaker in who_speaks_when.crop(first_minute).itertracks(yield_label=True):\n",
    "    waveform, sample_rate = audio.crop(audio_file, segment)\n",
    "    \n",
    "    # Convert to mono by averaging the two channels\n",
    "    if waveform.shape[0] == 2:  # Check if it's stereo\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)  # Average the two channels\n",
    "    \n",
    "    # Ensure waveform is a 1D tensor\n",
    "    waveform_tensor = waveform.squeeze()  # Remove any extra dimensions\n",
    "    \n",
    "    # Transcribe\n",
    "    text = model.transcribe(waveform_tensor.numpy())[\"text\"]  # Convert to numpy array\n",
    "    print(f\"{segment.start:06.1f}s {segment.end:06.1f}s {speaker}: {text}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_transcribe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
