{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 6.1.1 Copyright (c) 2000-2023 the FFmpeg developers\n",
      "  built with clang version 14.0.6\n",
      "  configuration: --prefix=/opt/anaconda3/envs/transcription --cc=arm64-apple-darwin20.0.0-clang --ar=arm64-apple-darwin20.0.0-ar --nm=arm64-apple-darwin20.0.0-nm --ranlib=arm64-apple-darwin20.0.0-ranlib --strip=arm64-apple-darwin20.0.0-strip --disable-doc --enable-swresample --enable-swscale --enable-openssl --enable-libxml2 --enable-libtheora --enable-demuxer=dash --enable-postproc --enable-hardcoded-tables --enable-libfreetype --enable-libharfbuzz --enable-libfontconfig --enable-libdav1d --enable-zlib --enable-libaom --enable-pic --enable-shared --disable-static --disable-gpl --enable-version3 --disable-sdl2 --enable-libopenh264 --enable-libopus --enable-libmp3lame --enable-libopenjpeg --enable-libvorbis --enable-pthreads --enable-libtesseract --enable-libvpx --enable-librsvg\n",
      "  libavutil      58. 29.100 / 58. 29.100\n",
      "  libavcodec     60. 31.102 / 60. 31.102\n",
      "  libavformat    60. 16.100 / 60. 16.100\n",
      "  libavdevice    60.  3.100 / 60.  3.100\n",
      "  libavfilter     9. 12.100 /  9. 12.100\n",
      "  libswscale      7.  5.100 /  7.  5.100\n",
      "  libswresample   4. 12.100 /  4. 12.100\n",
      "[mp3 @ 0x142604400] Estimating duration from bitrate, this may be inaccurate\n",
      "Input #0, mp3, from '/Users/lschoebitz/Documents/gitrepos/gh-larnsce/mywhisper/data/audio/meeting01/241118_1304.mp3':\n",
      "  Metadata:\n",
      "    title           : 241118_1304\n",
      "    artist          : My Recording\n",
      "    encoded_by      : SONY IC RECORDER 11.4.0\n",
      "  Duration: 00:00:36.02, start: 0.000000, bitrate: 192 kb/s\n",
      "  Stream #0:0: Audio: mp3, 44100 Hz, stereo, fltp, 192 kb/s\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mp3 (mp3float) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to '/Users/lschoebitz/Documents/gitrepos/gh-larnsce/mywhisper/data/audio/meeting01/241118_1304.wav':\n",
      "  Metadata:\n",
      "    INAM            : 241118_1304\n",
      "    IART            : My Recording\n",
      "    ITCH            : SONY IC RECORDER 11.4.0\n",
      "    ISFT            : Lavf60.16.100\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, stereo, s16, 1411 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc60.31.102 pcm_s16le\n",
      "[out#0/wav @ 0x142605600] video:0kB audio:6206kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.002392%\n",
      "size=    6206kB time=00:00:35.99 bitrate=1412.3kbits/s speed= 707x    \n"
     ]
    }
   ],
   "source": [
    "import ffmpeg\n",
    "\n",
    "# Example: Converting an MP3 file to WAV\n",
    "input_file = \"/Users/lschoebitz/Documents/gitrepos/gh-larnsce/mywhisper/data/audio/meeting01/241118_1304.mp3\"\n",
    "output_file = \"/Users/lschoebitz/Documents/gitrepos/gh-larnsce/mywhisper/data/audio/meeting01/241118_1304.wav\"\n",
    "\n",
    "try:\n",
    "    stream = ffmpeg.input(input_file)\n",
    "    stream = ffmpeg.output(stream, output_file)\n",
    "    ffmpeg.run(stream)\n",
    "    print(\"Conversion successful\")\n",
    "except ffmpeg.Error as e:\n",
    "    print(f\"Error: {e.stderr.decode()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.5.1. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
      "/opt/anaconda3/envs/transcription/lib/python3.10/site-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=device)\n",
      "/opt/anaconda3/envs/transcription/lib/python3.10/site-packages/speechbrain/processing/features.py:1311: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stats = torch.load(path, map_location=device)\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple Silicon GPU)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
      "/opt/anaconda3/envs/transcription/lib/python3.10/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "/opt/anaconda3/envs/transcription/lib/python3.10/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.63846875\n",
      "8.02971875\n",
      "0.63846875 8.02971875 \n",
      "0.63846875 8.02971875   So welcome to our interview today.\n",
      "0.63846875 8.02971875   So welcome to our interview today.  I'm here being the interviewer,\n",
      "8.02971875\n",
      "8.04659375\n",
      "9.059093750000002\n",
      "14.695343750000003\n",
      "16.24784375\n",
      "19.67346875\n",
      "20.41596875\n",
      "25.714718750000003\n",
      "20.41596875 25.714718750000003 \n",
      "26.94659375\n",
      "28.718468750000003\n",
      "30.06846875\n",
      "31.82346875\n",
      "32.88659375\n",
      "34.979093750000004\n",
      "Transcription results saved to 'transcription_results.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyannote.audio\n",
    "print(pyannote.audio.__version__)\n",
    "\n",
    "from pyannote.audio import Pipeline\n",
    "import whisper\n",
    "\n",
    "# Initialize Pyannote pipeline\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=\"\")\n",
    "\n",
    "# Load audio file\n",
    "audio_file = \"/Users/lschoebitz/Documents/gitrepos/gh-larnsce/mywhisper/data/audio/meeting01/241118_1304.wav\"\n",
    "\n",
    "import torch\n",
    "\n",
    "# Check if MPS is available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Send pipeline to the appropriate device\n",
    "pipeline.to(device)\n",
    "\n",
    "# Perform diarization\n",
    "diarization = pipeline(audio_file)\n",
    "\n",
    "# Load Whisper model\n",
    "model = whisper.load_model(\"base.en\")\n",
    "\n",
    "# Transcribe audio\n",
    "result = model.transcribe(audio_file)\n",
    "\n",
    "# Initialize an empty dictionary to store the results\n",
    "transcript = {}\n",
    "\n",
    "transcribed_sections = result[\"segments\"]\n",
    "\n",
    "# Process diarization and transcription results\n",
    "for segment, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    start = segment.start\n",
    "    end = segment.end\n",
    "\n",
    "    print(start)\n",
    "    print(end)\n",
    "\n",
    "    text = \"\"\n",
    "\n",
    "    for section in transcribed_sections:\n",
    "        if (section[\"start\"] >= np.floor(start) and section[\"end\"] <= np.ceil(end)):\n",
    "            print(start,end,text)\n",
    "            text += \" \" + section[\"text\"]\n",
    "\n",
    "\n",
    "    \n",
    "    # Append the text to the corresponding speaker in the dictionary\n",
    "    if speaker not in transcript:\n",
    "        transcript[speaker] = []\n",
    "    transcript[speaker].append((start, end, text))\n",
    "\n",
    "# Convert the dictionary into a list of records for DataFrame creation\n",
    "records = []\n",
    "for speaker, segments in transcript.items():\n",
    "    for start, end, text in segments:\n",
    "        records.append({\n",
    "            \"Speaker\": speaker,\n",
    "            \"Start\": start,\n",
    "            \"End\": end,\n",
    "            \"Text\": text\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the records\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"transcription_results.csv\", index=False)\n",
    "\n",
    "print(\"Transcription results saved to 'transcription_results.csv'.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transcription",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
