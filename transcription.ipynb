{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = \"/Users/ydubey/Desktop/transcription/241118_1543.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 6.1.1 Copyright (c) 2000-2023 the FFmpeg developers\n",
      "  built with clang version 14.0.6\n",
      "  configuration: --prefix=/opt/anaconda3/envs/transcription --cc=arm64-apple-darwin20.0.0-clang --ar=arm64-apple-darwin20.0.0-ar --nm=arm64-apple-darwin20.0.0-nm --ranlib=arm64-apple-darwin20.0.0-ranlib --strip=arm64-apple-darwin20.0.0-strip --disable-doc --enable-swresample --enable-swscale --enable-openssl --enable-libxml2 --enable-libtheora --enable-demuxer=dash --enable-postproc --enable-hardcoded-tables --enable-libfreetype --enable-libharfbuzz --enable-libfontconfig --enable-libdav1d --enable-zlib --enable-libaom --enable-pic --enable-shared --disable-static --disable-gpl --enable-version3 --disable-sdl2 --enable-libopenh264 --enable-libopus --enable-libmp3lame --enable-libopenjpeg --enable-libvorbis --enable-pthreads --enable-libtesseract --enable-libvpx --enable-librsvg\n",
      "  libavutil      58. 29.100 / 58. 29.100\n",
      "  libavcodec     60. 31.102 / 60. 31.102\n",
      "  libavformat    60. 16.100 / 60. 16.100\n",
      "  libavdevice    60.  3.100 / 60.  3.100\n",
      "  libavfilter     9. 12.100 /  9. 12.100\n",
      "  libswscale      7.  5.100 /  7.  5.100\n",
      "  libswresample   4. 12.100 /  4. 12.100\n",
      "[mp3 @ 0x152f164e0] Estimating duration from bitrate, this may be inaccurate\n",
      "Input #0, mp3, from '/Users/ydubey/Desktop/transcription/241118_1543.mp3':\n",
      "  Metadata:\n",
      "    title           : 241118_1543\n",
      "    artist          : My Recording\n",
      "    encoded_by      : SONY IC RECORDER 11.4.0\n",
      "  Duration: 00:00:44.64, start: 0.000000, bitrate: 192 kb/s\n",
      "  Stream #0:0: Audio: mp3, 44100 Hz, stereo, fltp, 192 kb/s\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mp3 (mp3float) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to '/Users/ydubey/Desktop/transcription/241118_1543.wav':\n",
      "  Metadata:\n",
      "    INAM            : 241118_1543\n",
      "    IART            : My Recording\n",
      "    ITCH            : SONY IC RECORDER 11.4.0\n",
      "    ISFT            : Lavf60.16.100\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, stereo, s16, 1411 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc60.31.102 pcm_s16le\n",
      "[out#0/wav @ 0x152f16d50] video:0kB audio:7690kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.001930%\n",
      "size=    7691kB time=00:00:44.61 bitrate=1412.1kbits/s speed= 746x    \n"
     ]
    }
   ],
   "source": [
    "import ffmpeg\n",
    "\n",
    "input_file = input_file_path\n",
    "output_file = input_file_path.replace(\".mp3\", \".wav\")\n",
    "\n",
    "try:\n",
    "    stream = ffmpeg.input(input_file)\n",
    "    stream = ffmpeg.output(stream, output_file)\n",
    "    ffmpeg.run(stream)\n",
    "    print(\"Conversion successful\")\n",
    "except ffmpeg.Error as e:\n",
    "    print(f\"Error: {e.stderr.decode()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.5.1. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
      "/opt/anaconda3/envs/transcription/lib/python3.10/site-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=device)\n",
      "/opt/anaconda3/envs/transcription/lib/python3.10/site-packages/speechbrain/processing/features.py:1311: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stats = torch.load(path, map_location=device)\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
      "/opt/anaconda3/envs/transcription/lib/python3.10/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "/opt/anaconda3/envs/transcription/lib/python3.10/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription results saved to 'transcription_results.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from pyannote.audio import Pipeline\n",
    "import whisper\n",
    "\n",
    "# Initialize Pyannote pipeline\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=\"YOUR HUGGING FACE TOKEN\")\n",
    "\n",
    "# Load audio file\n",
    "audio_file = output_file\n",
    "\n",
    "# Check if MPS is available and set device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "# Send pipeline to the appropriate device\n",
    "pipeline.to(device)\n",
    "\n",
    "# Perform diarization\n",
    "diarization = pipeline(audio_file)\n",
    "\n",
    "# Load Whisper model and transcribe audio\n",
    "model = whisper.load_model(\"base.en\")\n",
    "result = model.transcribe(audio_file)\n",
    "\n",
    "# Process diarization and transcription results\n",
    "transcript = {}\n",
    "assigned_sections = set()  # To track assigned sections\n",
    "\n",
    "def calculate_overlap(start1, end1, start2, end2):\n",
    "    \"\"\"Calculate the overlap between two time intervals.\"\"\"\n",
    "    overlap_start = max(start1, start2)\n",
    "    overlap_end = min(end1, end2)\n",
    "    return max(0, overlap_end - overlap_start)\n",
    "\n",
    "# Store overlaps for each section\n",
    "for segment, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    segment_duration = segment.end - segment.start\n",
    "    if speaker not in transcript:\n",
    "        transcript[speaker] = []\n",
    "\n",
    "    for section in result[\"segments\"]:\n",
    "        overlap = calculate_overlap(segment.start, segment.end, section[\"start\"], section[\"end\"])\n",
    "        overlap_fraction = overlap / (section[\"end\"] - section[\"start\"])\n",
    "\n",
    "        # Store overlap details\n",
    "        section.setdefault(\"overlaps\", [])\n",
    "        section[\"overlaps\"].append((speaker, overlap, overlap_fraction))\n",
    "\n",
    "# Assign text based on overlap conditions\n",
    "for section in result[\"segments\"]:\n",
    "    if \"overlaps\" in section and section[\"id\"] not in assigned_sections:\n",
    "        overlaps = sorted(section[\"overlaps\"], key=lambda x: x[1], reverse=True)  # Sort by overlap amount\n",
    "        max_overlap = overlaps[0]\n",
    "\n",
    "        if max_overlap[2] > 0.5:\n",
    "            # Assign to the segment with >50% overlap\n",
    "            speaker = max_overlap[0]\n",
    "            transcript[speaker].append((section[\"start\"], section[\"end\"], section[\"text\"]))\n",
    "            assigned_sections.add(section[\"id\"])\n",
    "        elif len(overlaps) > 1:\n",
    "            # Assign to the segment with the most overlap if all are <50%\n",
    "            speaker = max_overlap[0]\n",
    "            transcript[speaker].append((section[\"start\"], section[\"end\"], section[\"text\"]))\n",
    "            assigned_sections.add(section[\"id\"])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame([(speaker, start, end, text) \n",
    "                   for speaker, segments in transcript.items() \n",
    "                   for start, end, text in segments],\n",
    "                  columns=[\"Speaker\", \"Start\", \"End\", \"Text\"])\n",
    "\n",
    "# Sort by start time and save to CSV\n",
    "df.sort_values(by=\"Start\").reset_index(drop=True).to_csv(\"transcription_results_modified_3.csv\", index=False)\n",
    "\n",
    "print(\"Transcription results saved to 'transcription_results.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transcription",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
